{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7997e39d",
   "metadata": {},
   "source": [
    "## **Vanila SAC**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b08185",
   "metadata": {},
   "source": [
    "#### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bec61eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "from collections import deque\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RescaleAction\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d593b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action = 'ignore', category = UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1d52d2",
   "metadata": {},
   "source": [
    "#### **Device Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5116f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device working on: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f5c064",
   "metadata": {},
   "source": [
    "#### **Env Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcba83a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Humanoid-v5', max_episode_steps = 1000)\n",
    "\n",
    "env = RescaleAction(env, min_action = -1.0, max_action = 1.0)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = env.action_space.high[0]\n",
    "\n",
    "print(f'State dim: {state_dim}\\nAction dim: {action_dim} | Max Action range: {max_action}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9433bb",
   "metadata": {},
   "source": [
    "## **Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e972c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_extractor(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, h):\n",
    "        super(Feature_extractor, self).__init__()\n",
    "        \n",
    "        self.feature = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(input_dim, h),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(h),\n",
    "            nn.Linear(h,h),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(h,h),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            \n",
    "            nn.Linear(h,h),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(h),\n",
    "            nn.Linear(h,h),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            \n",
    "            nn.Linear(h,h),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(h),\n",
    "            nn.Linear(h,h),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(h,output_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "            x = self.feature(x)\n",
    "            \n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81081b6b",
   "metadata": {},
   "source": [
    "##### **Actor Design**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7383bd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_network(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, n1, n2, n3, n4, h1, max_action = max_action):\n",
    "        super(Actor_network, self).__init__()\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        \n",
    "        # Send to Feature Extractor\n",
    "        \n",
    "        self.feature = Feature_extractor(input_dim = state_dim, output_dim = n1, h = h1)\n",
    "        \n",
    "        # Introduction of MHA\n",
    "        \n",
    "        self.norm = nn.LayerNorm(n1)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim = n1, num_heads = 8, batch_first = True)\n",
    "        \n",
    "        # Actor network\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(n1, n2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(n2),\n",
    "            nn.Linear(n2, n3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(n3, n4)\n",
    "        )\n",
    "        \n",
    "        # Mean\n",
    "        \n",
    "        self.mu = nn.Linear(n4, action_dim)\n",
    "        \n",
    "        # Log Std\n",
    "        \n",
    "        self.log_std = nn.Linear(n4, action_dim)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        # Pass to feature\n",
    "        \n",
    "        feature = self.feature(state)\n",
    "        \n",
    "        # pass to norm\n",
    "        \n",
    "        x = self.norm(feature)\n",
    "        \n",
    "        # pass to MHA\n",
    "        x = x.unsqueeze(1)\n",
    "        x_2, _  = self.mha(x, x, x)\n",
    "        x_2 = x_2.squeeze(1)\n",
    "        \n",
    "        # pass to actor\n",
    "        \n",
    "        x_3 = self.actor(x_2)\n",
    "        \n",
    "        # Pass to mu\n",
    "        \n",
    "        mu = self.mu(x_3)\n",
    "        \n",
    "        # pass to log std\n",
    "        \n",
    "        log_std = self.log_std(x_3)\n",
    "        \n",
    "        # smooth scaling\n",
    "        \n",
    "        mu = torch.tanh_(mu)                # range[-1.0, 1.0]\n",
    "        log_std = torch.tanh_(log_std)      # range[-1.0, 1.0]\n",
    "        log_std = log_std.clamp(min = -5, max = 2)\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        \n",
    "        normal = torch.distributions.Normal(mu, std)\n",
    "        z = normal.rsample()\n",
    "        tanh_z = torch.tanh_(z)\n",
    "        log_prob = normal.log_prob(z)\n",
    "        action = tanh_z * self.max_action\n",
    "        \n",
    "        # Squashing\n",
    "        squash = 2 * (torch.log(torch.tensor(2.0, device=z.device)) - z - F.softplus(-2 * z))\n",
    "        log_prob = log_prob - squash\n",
    "        log_prob = torch.sum(log_prob, dim = 1, keepdim = True)\n",
    "        \n",
    "        return action, log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f66185",
   "metadata": {},
   "source": [
    "##### **Critic Design**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb58c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic_network(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, n1, n2, n3, n4, h1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pass to feature\n",
    "        \n",
    "        self.feature = Feature_extractor(state_dim + action_dim, n1, h1)\n",
    "        \n",
    "        # pass to mha\n",
    "        \n",
    "        self.norm = nn.LayerNorm(n1)\n",
    "        self.mha = nn.MultiheadAttention(n1, 8, batch_first = True)\n",
    "        \n",
    "        # pass to critic\n",
    "        \n",
    "        # critic 1\n",
    "        \n",
    "        self.critic_1 = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(n1, n2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(n2),\n",
    "            nn.Linear(n2, n3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(n3, n4),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(n4, 1)\n",
    "        )\n",
    "        \n",
    "        # critic 2\n",
    "        \n",
    "        self.critic_2 = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(n1, n2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(n2),\n",
    "            nn.Linear(n2, n3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(n3, n4),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(n4, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \n",
    "        # shape check\n",
    "        \n",
    "        if state.dim() == 3:\n",
    "            state = state.squeeze(-1)\n",
    "            \n",
    "        if action.dim() == 3:\n",
    "            action = action.squeeze(-1)\n",
    "            \n",
    "        x = torch.cat([state, action], dim = -1)\n",
    "        \n",
    "        # Pass to feature\n",
    "        \n",
    "        feature = self.feature(x)\n",
    "        \n",
    "        # Pass to MHA\n",
    "        \n",
    "        x_2 = self.norm(feature)\n",
    "        x_2 = x_2.unsqueeze(1)\n",
    "        x_3, _ = self.mha(x_2, x_2, x_2)\n",
    "        x_3 = x_3.squeeze(1)\n",
    "        \n",
    "        # Pass to critic\n",
    "        \n",
    "        q_1 = self.critic_1(x_3)\n",
    "        q_2 = self.critic_2(x_3)\n",
    "        \n",
    "        return q_1, q_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a81555",
   "metadata": {},
   "source": [
    "#### **Set up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e52a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembly of neurons\n",
    "\n",
    "n1 = 128\n",
    "h1 = 128\n",
    "n2 = 256\n",
    "n3 = 512\n",
    "n4 = 256\n",
    "\n",
    "# Initialize\n",
    "\n",
    "actor_network = Actor_network(state_dim, action_dim, n1, n2, n3, n4, h1).to(device)\n",
    "\n",
    "print(actor_network)\n",
    "\n",
    "print('-------------------------------------------------------------------------------')\n",
    "\n",
    "critic_network = Critic_network(state_dim, action_dim, n1, n2, n3, n4, h1).to(device)\n",
    "\n",
    "print(critic_network)\n",
    "\n",
    "\n",
    "# target network\n",
    "target_critic = copy.deepcopy(critic_network).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f0bab1",
   "metadata": {},
   "source": [
    "### **Soft Update**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c921a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(source, target, tau = 0.067):\n",
    "    \n",
    "    for param , target_param in zip(source.parameters(), target.parameters()):\n",
    "        \n",
    "        target_param.data.copy_(tau * param.data + (1-tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b96a66",
   "metadata": {},
   "source": [
    "## **Agent Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090b12d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC_Agent:\n",
    "    \n",
    "    def __init__(self, actor_network, critic_network, target_critic, actor_optimizer, actor_scheduler, critic_optimizer, critic_scheduler, gamma):\n",
    "        \n",
    "        # Define networks\n",
    "        \n",
    "        self.actor = actor_network\n",
    "        self.critic = critic_network\n",
    "        self.target_critic = target_critic\n",
    "        \n",
    "        # Define hyperparams\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Define optimizer and scheduler\n",
    "        \n",
    "        self.actor_optimizer = actor_optimizer\n",
    "        self.actor_scheduler = actor_scheduler\n",
    "        self.critic_optimizer = critic_optimizer\n",
    "        self.critic_scheduler = critic_scheduler\n",
    "        \n",
    "        # Auto tuning\n",
    "        \n",
    "        self.target_entropy = - action_dim * 1.5\n",
    "        self.log_alpha = torch.tensor(np.log(0.2), requires_grad = True, device = device)\n",
    "        self.alpha_min = 0.1\n",
    "        self.alpha_optimizer = optim.AdamW([self.log_alpha], lr = 1e-4, weight_decay = 0.001)\n",
    "        \n",
    "    def compute_alpha(self):\n",
    "        \n",
    "        self.alpha = self.log_alpha.exp().detach()\n",
    "        self.alpha = self.alpha.clamp(min = self.alpha_min, max = 0.2)\n",
    "        \n",
    "        return self.alpha\n",
    "    \n",
    "    def update_target(self):\n",
    "        \n",
    "        return soft_update(self.critic, self.target_critic)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \n",
    "        state = torch.tensor(state, dtype = torch.float32).to(device)\n",
    "        \n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        \n",
    "        action, log_prob = self.actor(state)\n",
    "        \n",
    "        return action, log_prob\n",
    "        \n",
    "    def update(self, replay_buffer, batch_size):\n",
    "        \n",
    "        state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "               \n",
    "        # Move to device\n",
    "        \n",
    "        state = state.to(device)\n",
    "        action = action.to(device)\n",
    "        reward = reward.to(device)\n",
    "        next_state = next_state.to(device)\n",
    "        done = done.to(device)\n",
    "        \n",
    "        # compute target q value\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            next_action, _ = self.select_action(next_state)\n",
    "            \n",
    "            if next_action.dim() == 1:\n",
    "                next_action.unsqueeze(0)\n",
    "            \n",
    "            target_1, target_2 = self.target_critic(next_state, next_action)\n",
    "            target_Q = ((0.75 * torch.min(target_1, target_2)) + (0.25 * torch.max(target_1, target_2)))\n",
    "            target_value = reward + self.gamma * (1 - done) * target_Q\n",
    "            \n",
    "            \n",
    "        # Compute current q value\n",
    "        \n",
    "        current_q1, current_q2 = self.critic(state, action)\n",
    "        critic_loss_1 = F.mse_loss(current_q1, target_value)\n",
    "        critic_loss_2 = F.mse_loss(current_q2, target_value)\n",
    "        \n",
    "        critic_loss = critic_loss_1 + critic_loss_2\n",
    "        \n",
    "        # Update critic\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm = 0.5)\n",
    "        self.critic_optimizer.step() \n",
    "        self.critic_scheduler.step()\n",
    "        \n",
    "        \n",
    "        # compute actor loss\n",
    "        \n",
    "        new_action, new_log_prob = self.actor(state)\n",
    "        old_log_prob = new_log_prob.detach()\n",
    "        \n",
    "        q_1, q_2 = self.critic(state, new_action)\n",
    "        q_pi = ((0.75 * torch.min(q_1, q_2)) + (0.25 * torch.max(q_1, q_2)))\n",
    "        \n",
    "        #Kl_Div =  torch.sum(torch.exp(old_log_prob) * (old_log_prob - new_log_prob), dim = -1).mean()\n",
    "        \n",
    "        actor_loss = ((self.compute_alpha() * new_log_prob - q_pi)).mean()\n",
    "\n",
    "        # Update Actor network\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm = 0.5)\n",
    "        self.actor_optimizer.step()\n",
    "        self.actor_scheduler.step()\n",
    "        \n",
    "        \n",
    "        self.update_target()\n",
    "        \n",
    "        # Alpha optimizer\n",
    "        \n",
    "        alpha_loss = - (self.log_alpha * (old_log_prob + self.target_entropy)).mean()\n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_([self.log_alpha], max_norm = 1.0)\n",
    "        self.alpha_optimizer.step()\n",
    "        \n",
    "        return actor_loss.item(), critic_loss.item(), self.alpha.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78059b7",
   "metadata": {},
   "source": [
    "#### **Tensor Conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c0f196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_tensor(x):\n",
    "    \n",
    "    return x if torch.is_tensor(x) else torch.tensor(x, dtype = torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97564c5",
   "metadata": {},
   "source": [
    "### **Replay Buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdfd97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC_Buffer:\n",
    "    \n",
    "    def __init__(self, capacity, batch_size):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.sac_buffer = deque(maxlen = capacity)\n",
    "        self.ptr = 0\n",
    "        \n",
    "    def add(self, state,  action, reward, next_state, done):\n",
    "        \n",
    "        # Converting to tensor\n",
    "        \n",
    "        state = safe_tensor(state)\n",
    "        action = safe_tensor(action)\n",
    "        reward = safe_tensor(reward)\n",
    "        next_state = safe_tensor(next_state)\n",
    "        done = safe_tensor(done)\n",
    "        \n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        \n",
    "        if len(self.sac_buffer) < self.capacity:\n",
    "            \n",
    "            self.sac_buffer.append(experience)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.sac_buffer[self.ptr] = experience\n",
    "            self.ptr = (1 + self.ptr) % self.capacity\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.sac_buffer)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \n",
    "        indices = np.random.choice(len(self.sac_buffer), batch_size)\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = zip(*[self.sac_buffer[ind] for ind in indices])\n",
    "        \n",
    "        # convert to tensor\n",
    "        \n",
    "        states = torch.stack(states).to(device)\n",
    "        actions = torch.stack(actions).to(device)\n",
    "        rewards = torch.stack(rewards).to(device)\n",
    "        next_states = torch.stack(next_states).to(device)\n",
    "        dones = torch.stack(dones).to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ceb0bb",
   "metadata": {},
   "source": [
    "#### **Set up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63708c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_buffer = SAC_Buffer(capacity = 500_000, batch_size = 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b1b325",
   "metadata": {},
   "source": [
    "### **Hyper Params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852988d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper params\n",
    "\n",
    "gamma = 0.997\n",
    "max_iter = 10_000\n",
    "\n",
    "# Actor params\n",
    "\n",
    "actor_optimizer = optim.AdamW(actor_network.parameters(), lr = 3e-4, weight_decay = 0.001)\n",
    "actor_scheduler = optim.lr_scheduler.CosineAnnealingLR(actor_optimizer, T_max = max_iter)\n",
    "\n",
    "# Critic params\n",
    "\n",
    "critic_optimizer = optim.AdamW(critic_network.parameters(), lr = 1e-4, weight_decay = 0.001)\n",
    "critic_scheduler = optim.lr_scheduler.CosineAnnealingLR(critic_optimizer, T_max = max_iter)\n",
    "\n",
    "\n",
    "# SAC agent\n",
    "agent = SAC_Agent(actor_network, critic_network, target_critic, actor_optimizer, actor_scheduler, critic_optimizer, critic_scheduler, gamma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
