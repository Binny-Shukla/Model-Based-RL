{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3596476d",
   "metadata": {},
   "source": [
    "# **Model Based Policy Optimization**\n",
    "\n",
    "Integrating Enhanced SAC to a world model and training sac on rollouts so it will be actually dreaming while the world model interacts with the env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9eed534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "from collections import deque\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RescaleAction\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "042f7f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action = 'ignore', category = UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7550a364",
   "metadata": {},
   "source": [
    "##  **Device Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "12b9d15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device working on: cuda\n"
     ]
    }
   ],
   "source": [
    "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device working on: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be489115",
   "metadata": {},
   "source": [
    "## *Env Setup*\n",
    "\n",
    "We will be working on **Walker 2d** so it is easy to benchmarks against **pytorch and tensorflow** implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1f3479d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dim: 348\n",
      "Action dim: 17 | Max Action range: 1.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Humanoid-v5', max_episode_steps = 1000)\n",
    "\n",
    "env = RescaleAction(env, min_action = -1.0, max_action = 1.0)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = env.action_space.high[0]\n",
    "\n",
    "print(f'State dim: {state_dim}\\nAction dim: {action_dim} | Max Action range: {max_action}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b6faa3",
   "metadata": {},
   "source": [
    "# **World Model**\n",
    "\n",
    "Model which interacts with the enviornment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df834a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class World_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_1, head_2, head_3, head_4, state_dim = state_dim, action_dim = action_dim):\n",
    "        super(World_Model, self).__init__()\n",
    "        \n",
    "        self.world = nn.Sequential(\n",
    "            \n",
    "            # first Layer combination of state and action dim \n",
    "            \n",
    "            nn.Linear(state_dim + action_dim, head_1),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            # Layer Norm for stabilization and proceeding with increment bottleneck technqiue\n",
    "            \n",
    "            nn.LayerNorm(head_1),\n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            \n",
    "            # third layer no layer norm for perfect gradients flow without major resitriction\n",
    "            \n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            \n",
    "            # Forth layer cause head 3 will be a major assembly of neuron\n",
    "            \n",
    "            nn.LayerNorm(head_3),\n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            \n",
    "            # Output and final layer\n",
    "            \n",
    "            nn.Linear(head_4, state_dim + 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \n",
    "        x = torch.cat([state, action], dim = -1)\n",
    "        \n",
    "        x_2 = self.world(x)\n",
    "        \n",
    "        next_state = x_2[:, :-1]\n",
    "        reward = x_2[:, -1:]\n",
    "        \n",
    "        return next_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a48c76c",
   "metadata": {},
   "source": [
    "## **World Model 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ddab63f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class World_Model_3(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_1, head_2, head_3, head_4, state_dim = state_dim, action_dim = action_dim):\n",
    "        super(World_Model_3, self).__init__()\n",
    "        \n",
    "        self.projection = nn.Linear(state_dim + action_dim, head_1)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(head_1)\n",
    "        \n",
    "        self.MHA = nn.MultiheadAttention(head_1, num_heads = 8, batch_first = True)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(head_1, head_2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(head_2),\n",
    "            nn.Linear(head_2, head_3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_3, head_4),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(head_4, state_dim + 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \n",
    "        x = torch.cat([state, action], dim = -1)\n",
    "        \n",
    "        x_2 = self.projection(x)\n",
    "        x_3 = self.norm(x_2)\n",
    "        \n",
    "        # Preparing for MHA\n",
    "        \n",
    "        x_3 = x_3.unsqueeze(1)\n",
    "        attn, _ = self.MHA(x_3, x_3, x_3)\n",
    "        \n",
    "        # Preparing for FC \n",
    "        \n",
    "        attn = attn.squeeze(1)\n",
    "        \n",
    "        x_4 = self.fc(attn)\n",
    "        \n",
    "        next_state = x_4[:, :-1]\n",
    "        reward = x_4[:, -1:]\n",
    "        \n",
    "        return next_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd34c8f2",
   "metadata": {},
   "source": [
    "#### **Initialize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "77dbf60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World_Model(\n",
      "  (world): Sequential(\n",
      "    (0): Linear(in_features=365, out_features=128, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (4): SiLU()\n",
      "    (5): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (6): SiLU()\n",
      "    (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (8): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (9): SiLU()\n",
      "    (10): Linear(in_features=256, out_features=349, bias=True)\n",
      "  )\n",
      ")\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "World_Model(\n",
      "  (world): Sequential(\n",
      "    (0): Linear(in_features=365, out_features=128, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (4): SiLU()\n",
      "    (5): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (6): SiLU()\n",
      "    (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (8): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (9): SiLU()\n",
      "    (10): Linear(in_features=256, out_features=349, bias=True)\n",
      "  )\n",
      ")\n",
      " -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  - \n",
      "World_Model_3(\n",
      "  (projection): Linear(in_features=365, out_features=128, bias=True)\n",
      "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (MHA): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (4): SiLU()\n",
      "    (5): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (6): SiLU()\n",
      "    (7): Linear(in_features=256, out_features=349, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Neuron assembly \n",
    "\n",
    "head_1 = 128\n",
    "head_2 = 256\n",
    "head_3 = 512\n",
    "head_4 = 256\n",
    "\n",
    "# world Model \n",
    "\n",
    "world_model = World_Model(head_1, head_2, head_3, head_4).to(device)\n",
    "\n",
    "print(world_model)\n",
    "\n",
    "# World Model 2 (Using an ensemble of model)\n",
    "\n",
    "world_model_2 = copy.deepcopy(world_model).to(device)\n",
    "\n",
    "print(\" - \" * 80)\n",
    "print(world_model_2)\n",
    "\n",
    "# World model 3\n",
    "\n",
    "world_model_3 = World_Model_3(head_1, head_2, head_3, head_4).to(device)\n",
    "\n",
    "print(\" - \" * 80)\n",
    "print(world_model_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22eb628",
   "metadata": {},
   "source": [
    "## **Ensemble**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "35e28820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class world_model_ensemble(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_1, model_2, model_3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.models = nn.ModuleList([\n",
    "            \n",
    "            model_1,\n",
    "            model_2, \n",
    "            model_3\n",
    "        ])\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \n",
    "        pred = [model(state, action) for model in self.models]\n",
    "        \n",
    "        next_state = torch.stack([p[0] for p in pred], dim = 0)\n",
    "        \n",
    "        reward = torch.stack([p[1] for p in pred], dim = 0)\n",
    "    \n",
    "        return next_state.mean(dim = 0), reward.mean(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ed085773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world_model_ensemble(\n",
      "  (models): ModuleList(\n",
      "    (0-1): 2 x World_Model(\n",
      "      (world): Sequential(\n",
      "        (0): Linear(in_features=365, out_features=128, bias=True)\n",
      "        (1): SiLU()\n",
      "        (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (3): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (4): SiLU()\n",
      "        (5): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (6): SiLU()\n",
      "        (7): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (8): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (9): SiLU()\n",
      "        (10): Linear(in_features=256, out_features=349, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): World_Model_3(\n",
      "      (projection): Linear(in_features=365, out_features=128, bias=True)\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (MHA): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (1): SiLU()\n",
      "        (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (3): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (4): SiLU()\n",
      "        (5): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (6): SiLU()\n",
      "        (7): Linear(in_features=256, out_features=349, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ensemble = world_model_ensemble(world_model, world_model_2, world_model_3).to(device)\n",
    "\n",
    "print(ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe243809",
   "metadata": {},
   "source": [
    "# **Dynamic Buffer**\n",
    "\n",
    "It will be saving the trajectories for the world model interacting with the env which will be then used in cut mix augmentation and training SAC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2ef4aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class World_Buffer():\n",
    "    \n",
    "    def __init__(self, capacity, batch_size):\n",
    "        \n",
    "        self.ptr = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = deque(maxlen = capacity)\n",
    "        self.capacity = capacity\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # convert to tensors\n",
    "        \n",
    "        state = torch.tensor(state, dtype = torch.float32).to(device)\n",
    "        next_state = torch.tensor(next_state, dtype = torch.float32).to(device)\n",
    "        action = torch.tensor(action, dtype = torch.float32).to(device)\n",
    "        reward = torch.tensor(float(reward), dtype = torch.float32).to(device)\n",
    "        done = torch.tensor(float(done), dtype = torch.float32).to(device)\n",
    "        \n",
    "        \n",
    "        experience = (state, action, reward, next_state, done)           # S A R S A\n",
    "        \n",
    "        # If len self.buffer = capacity\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            \n",
    "            self.buffer.append(experience)\n",
    "            \n",
    "        else:\n",
    "            self.buffer[self.ptr] = experience\n",
    "            self.ptr = (1 + self.ptr) % self.capacity                    # Roll out for random attentions\n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.buffer)\n",
    "    \n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size)\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = zip(*[self.buffer[ind] for ind in indices])\n",
    "        \n",
    "        # Safe conversion to tensor \n",
    "        \n",
    "        states = torch.stack(states).to(device)\n",
    "        actions = torch.stack(actions).to(device)\n",
    "        rewards = torch.stack(rewards).to(device)\n",
    "        next_states = torch.stack(next_states).to(device)\n",
    "        dones = torch.stack(dones).to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dabb5f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_buffer = World_Buffer(capacity = 500_000, batch_size = 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a8eb4b",
   "metadata": {},
   "source": [
    "# **World Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bfbb215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class World_loss():\n",
    "    \n",
    "    def __init__(self, world_optimizer, world_scheduler, ensemble):\n",
    "        \n",
    "        self.model = ensemble\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.optimizer = world_optimizer\n",
    "        self.scheduler = world_scheduler\n",
    "        \n",
    "    def compute_loss(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # Safe Convert them to tensor\n",
    "\n",
    "        \n",
    "        state = state.to(device)\n",
    "        action = action.to(device)\n",
    "        reward = reward.to(device)\n",
    "        next_state = next_state.to(device)\n",
    "        done = done.to(device)\n",
    "        \n",
    "        # Shape check\n",
    "        \n",
    "        if state.dim() == 3:\n",
    "            state = state.squeeze(1)\n",
    "        \n",
    "        if action.dim() == 3:\n",
    "            action = action.squeeze(1)\n",
    "            \n",
    "        # Predict next state and next reward\n",
    "        \n",
    "        pred_state, pred_reward = self.model(state, action)\n",
    "        \n",
    "        # compute loss\n",
    "        \n",
    "        state_loss = self.loss_func(pred_state, next_state)\n",
    "        reward_loss = self.loss_func(pred_reward.squeeze(-1), reward)\n",
    "        total_loss = state_loss + reward_loss\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm = 0.5)\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        return total_loss.item(), state_loss.item(), reward_loss.item()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb6da39",
   "metadata": {},
   "source": [
    "#### **Initialize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6282b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optimizer\n",
    "\n",
    "world_optimizer = optim.AdamW(world_model.parameters(), lr = 1e-4, weight_decay = 0.001)\n",
    "\n",
    "# Scheduler \n",
    "\n",
    "world_scheduler = optim.lr_scheduler.CosineAnnealingLR(world_optimizer, T_max = 30)\n",
    "\n",
    "# Loss\n",
    "\n",
    "world_loss = World_loss(world_optimizer, world_scheduler, world_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fc8e35",
   "metadata": {},
   "source": [
    "# **SAC Agent**\n",
    "\n",
    "This is an different SAC approach and enhanced approach used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0d364f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_extractor(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, h):\n",
    "        super(Feature_extractor, self).__init__()\n",
    "        \n",
    "        self.feature = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(input_dim, h),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(h),\n",
    "            nn.Linear(h,h),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(h,h),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            \n",
    "            nn.Linear(h,h),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(h),\n",
    "            nn.Linear(h,h),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            \n",
    "            nn.Linear(h,h),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(h),\n",
    "            nn.Linear(h,h),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(h,output_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "            x = self.feature(x)\n",
    "            \n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "331eb0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_network(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, n1, n2, n3, n4, h1, max_action = max_action):\n",
    "        super(Actor_network, self).__init__()\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        \n",
    "        # Send to Feature Extractor\n",
    "        \n",
    "        self.feature = Feature_extractor(input_dim = state_dim, output_dim = n1, h = h1)\n",
    "        \n",
    "        # Introduction of MHA\n",
    "        \n",
    "        self.norm = nn.LayerNorm(n1)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim = n1, num_heads = 8, batch_first = True)\n",
    "        \n",
    "        # Actor network\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(n1, n2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(n2),\n",
    "            nn.Linear(n2, n3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(n3, n4)\n",
    "        )\n",
    "        \n",
    "        # Mean\n",
    "        \n",
    "        self.mu = nn.Linear(n4, action_dim)\n",
    "        \n",
    "        # Log Std\n",
    "        \n",
    "        self.log_std = nn.Linear(n4, action_dim)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        # Pass to feature\n",
    "        \n",
    "        feature = self.feature(state)\n",
    "        \n",
    "        # pass to norm\n",
    "        \n",
    "        x = self.norm(feature)\n",
    "        \n",
    "        # pass to MHA\n",
    "        x = x.unsqueeze(1)\n",
    "        x_2, _  = self.mha(x, x, x)\n",
    "        x_2 = x_2.squeeze(1)\n",
    "        \n",
    "        # pass to actor\n",
    "        \n",
    "        x_3 = self.actor(x_2)\n",
    "        \n",
    "        # Pass to mu\n",
    "        \n",
    "        mu = self.mu(x_3)\n",
    "        \n",
    "        # pass to log std\n",
    "        \n",
    "        log_std = self.log_std(x_3)\n",
    "        \n",
    "        # smooth scaling\n",
    "        \n",
    "        mu = torch.tanh_(mu)                # range[-1.0, 1.0]\n",
    "        log_std = torch.tanh_(log_std)      # range[-1.0, 1.0]\n",
    "        log_std = log_std.clamp(min = -5, max = 2)\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        \n",
    "        normal = torch.distributions.Normal(mu, std)\n",
    "        z = normal.rsample()\n",
    "        tanh_z = torch.tanh_(z)\n",
    "        log_prob = normal.log_prob(z)\n",
    "        action = tanh_z * self.max_action\n",
    "        \n",
    "        # Squashing\n",
    "        squash = 2 * (torch.log(torch.tensor(2.0, device=z.device)) - z - F.softplus(-2 * z))\n",
    "        log_prob = log_prob - squash\n",
    "        log_prob = torch.sum(log_prob, dim = 1, keepdim = True)\n",
    "        \n",
    "        return action, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d8978587",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic_network(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, n1, n2, n3, n4, h1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Pass to feature\n",
    "        \n",
    "        self.feature = Feature_extractor(state_dim + action_dim, n1, h1)\n",
    "        \n",
    "        # pass to mha\n",
    "        \n",
    "        self.norm = nn.LayerNorm(n1)\n",
    "        self.mha = nn.MultiheadAttention(n1, 8, batch_first = True)\n",
    "        \n",
    "        # pass to critic\n",
    "        \n",
    "        # critic 1\n",
    "        \n",
    "        self.critic_1 = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(n1, n2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(n2),\n",
    "            nn.Linear(n2, n3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(n3, n4),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(n4, 1)\n",
    "        )\n",
    "        \n",
    "        # critic 2\n",
    "        \n",
    "        self.critic_2 = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(n1, n2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.LayerNorm(n2),\n",
    "            nn.Linear(n2, n3),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(n3, n4),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Linear(n4, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \n",
    "        # shape check\n",
    "        \n",
    "        if state.dim() == 3:\n",
    "            state = state.squeeze(-1)\n",
    "            \n",
    "        if action.dim() == 3:\n",
    "            action = action.squeeze(-1)\n",
    "            \n",
    "        x = torch.cat([state, action], dim = -1)\n",
    "        \n",
    "        # Pass to feature\n",
    "        \n",
    "        feature = self.feature(x)\n",
    "        \n",
    "        # Pass to MHA\n",
    "        \n",
    "        x_2 = self.norm(feature)\n",
    "        x_2 = x_2.unsqueeze(1)\n",
    "        x_3, _ = self.mha(x_2, x_2, x_2)\n",
    "        x_3 = x_3.squeeze(1)\n",
    "        \n",
    "        # Pass to critic\n",
    "        \n",
    "        q_1 = self.critic_1(x_3)\n",
    "        q_2 = self.critic_2(x_3)\n",
    "        \n",
    "        return q_1, q_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b128a5",
   "metadata": {},
   "source": [
    "####  **Initialize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fcb3f3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor_network(\n",
      "  (feature): Feature_extractor(\n",
      "    (feature): Sequential(\n",
      "      (0): Linear(in_features=348, out_features=128, bias=True)\n",
      "      (1): SiLU()\n",
      "      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (4): SiLU()\n",
      "      (5): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (6): SiLU()\n",
      "      (7): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (8): SiLU()\n",
      "      (9): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (10): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (11): SiLU()\n",
      "      (12): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (13): SiLU()\n",
      "      (14): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (15): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (16): SiLU()\n",
      "      (17): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (18): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (4): SiLU()\n",
      "    (5): Linear(in_features=512, out_features=256, bias=True)\n",
      "  )\n",
      "  (mu): Linear(in_features=256, out_features=17, bias=True)\n",
      "  (log_std): Linear(in_features=256, out_features=17, bias=True)\n",
      ")\n",
      "-------------------------------------------------------------------------------\n",
      "Critic_network(\n",
      "  (feature): Feature_extractor(\n",
      "    (feature): Sequential(\n",
      "      (0): Linear(in_features=365, out_features=128, bias=True)\n",
      "      (1): SiLU()\n",
      "      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (4): SiLU()\n",
      "      (5): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (6): SiLU()\n",
      "      (7): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (8): SiLU()\n",
      "      (9): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (10): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (11): SiLU()\n",
      "      (12): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (13): SiLU()\n",
      "      (14): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (15): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (16): SiLU()\n",
      "      (17): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (18): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (critic_1): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (4): SiLU()\n",
      "    (5): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (6): SiLU()\n",
      "    (7): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (critic_2): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (4): SiLU()\n",
      "    (5): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (6): SiLU()\n",
      "    (7): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Assembly of neurons\n",
    "\n",
    "n1 = 128\n",
    "h1 = 128\n",
    "n2 = 256\n",
    "n3 = 512\n",
    "n4 = 256\n",
    "\n",
    "# Initialize\n",
    "\n",
    "actor_network = Actor_network(state_dim, action_dim, n1, n2, n3, n4, h1).to(device)\n",
    "\n",
    "print(actor_network)\n",
    "\n",
    "print('-------------------------------------------------------------------------------')\n",
    "\n",
    "critic_network = Critic_network(state_dim, action_dim, n1, n2, n3, n4, h1).to(device)\n",
    "\n",
    "print(critic_network)\n",
    "\n",
    "\n",
    "# target network\n",
    "target_critic = copy.deepcopy(critic_network).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eca4fc",
   "metadata": {},
   "source": [
    "### **Soft Update**\n",
    "\n",
    "For smooth updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4b418de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(source, target, tau = 0.067):\n",
    "    \n",
    "    for param , target_param in zip(source.parameters(), target.parameters()):\n",
    "        \n",
    "        target_param.data.copy_(tau * param.data + (1-tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197e0fbe",
   "metadata": {},
   "source": [
    "# **Sac_Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f707de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC_Agent:\n",
    "    \n",
    "    def __init__(self, actor_network, critic_network, target_critic, actor_optimizer, actor_scheduler, critic_optimizer, critic_scheduler, gamma):\n",
    "        \n",
    "        # Define networks\n",
    "        \n",
    "        self.actor = actor_network\n",
    "        self.critic = critic_network\n",
    "        self.target_critic = target_critic\n",
    "        \n",
    "        # Define hyperparams\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Define optimizer and scheduler\n",
    "        \n",
    "        self.actor_optimizer = actor_optimizer\n",
    "        self.actor_scheduler = actor_scheduler\n",
    "        self.critic_optimizer = critic_optimizer\n",
    "        self.critic_scheduler = critic_scheduler\n",
    "        \n",
    "        # Auto tuning\n",
    "        \n",
    "        self.target_entropy = - action_dim * 1.5\n",
    "        self.log_alpha = torch.tensor(np.log(0.2), requires_grad = True, device = device)\n",
    "        self.alpha_min = 0.1\n",
    "        self.alpha_optimizer = optim.AdamW([self.log_alpha], lr = 1e-4, weight_decay = 0.001)\n",
    "        \n",
    "    def compute_alpha(self):\n",
    "        \n",
    "        self.alpha = self.log_alpha.exp().detach()\n",
    "        self.alpha = self.alpha.clamp(min = self.alpha_min, max = 0.2)\n",
    "        \n",
    "        return self.alpha\n",
    "    \n",
    "    def update_target(self):\n",
    "        \n",
    "        return soft_update(self.critic, self.target_critic)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \n",
    "        state = torch.tensor(state, dtype = torch.float32).to(device)\n",
    "        \n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        \n",
    "        action, log_prob = self.actor(state)\n",
    "        \n",
    "        return action, log_prob\n",
    "        \n",
    "    def update(self, replay_buffer, batch_size):\n",
    "        \n",
    "        state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "               \n",
    "        # Move to device\n",
    "        \n",
    "        state = state.to(device)\n",
    "        action = action.to(device)\n",
    "        reward = reward.to(device)\n",
    "        next_state = next_state.to(device)\n",
    "        done = done.to(device)\n",
    "        \n",
    "        # compute target q value\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            next_action, _ = self.select_action(next_state)\n",
    "            \n",
    "            if next_action.dim() == 1:\n",
    "                next_action.unsqueeze(0)\n",
    "            \n",
    "            target_1, target_2 = self.target_critic(next_state, next_action)\n",
    "            target_Q = ((0.75 * torch.min(target_1, target_2)) + (0.25 * torch.max(target_1, target_2)))\n",
    "            target_value = reward + self.gamma * (1 - done) * target_Q\n",
    "            \n",
    "            \n",
    "        # Compute current q value\n",
    "        \n",
    "        current_q1, current_q2 = self.critic(state, action)\n",
    "        critic_loss_1 = F.mse_loss(current_q1, target_value)\n",
    "        critic_loss_2 = F.mse_loss(current_q2, target_value)\n",
    "        \n",
    "        critic_loss = critic_loss_1 + critic_loss_2\n",
    "        \n",
    "        # Update critic\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm = 0.5)\n",
    "        self.critic_optimizer.step() \n",
    "        self.critic_scheduler.step()\n",
    "        \n",
    "        \n",
    "        # compute actor loss\n",
    "        \n",
    "        new_action, new_log_prob = self.actor(state)\n",
    "        old_log_prob = new_log_prob.detach()\n",
    "        \n",
    "        q_1, q_2 = self.critic(state, new_action)\n",
    "        q_pi = ((0.75 * torch.min(q_1, q_2)) + (0.25 * torch.max(q_1, q_2)))\n",
    "        \n",
    "        #Kl_Div =  torch.sum(torch.exp(old_log_prob) * (old_log_prob - new_log_prob), dim = -1).mean()\n",
    "        \n",
    "        actor_loss = ((self.compute_alpha() * new_log_prob - q_pi)).mean()\n",
    "\n",
    "        # Update Actor network\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm = 0.5)\n",
    "        self.actor_optimizer.step()\n",
    "        self.actor_scheduler.step()\n",
    "        \n",
    "        \n",
    "        self.update_target()\n",
    "        \n",
    "        # Alpha optimizer\n",
    "        \n",
    "        alpha_loss = - (self.log_alpha * (old_log_prob + self.target_entropy)).mean()\n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_([self.log_alpha], max_norm = 1.0)\n",
    "        self.alpha_optimizer.step()\n",
    "        \n",
    "        return actor_loss.item(), critic_loss.item(), self.alpha.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd61f8f",
   "metadata": {},
   "source": [
    "#### *Reducing CodeFrame*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f85fa3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_tensor(x):\n",
    "    \n",
    "    return x if torch.is_tensor(x) else torch.tensor(x, dtype = torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63474d21",
   "metadata": {},
   "source": [
    "## **SAC Buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "26619b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC_Buffer:\n",
    "    \n",
    "    def __init__(self, capacity, batch_size):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.sac_buffer = deque(maxlen = capacity)\n",
    "        self.ptr = 0\n",
    "        \n",
    "    def add(self, state,  action, reward, next_state, done):\n",
    "        \n",
    "        # Converting to tensor\n",
    "        \n",
    "        state = safe_tensor(state)\n",
    "        action = safe_tensor(action)\n",
    "        reward = safe_tensor(reward)\n",
    "        next_state = safe_tensor(next_state)\n",
    "        done = safe_tensor(done)\n",
    "        \n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        \n",
    "        if len(self.sac_buffer) < self.capacity:\n",
    "            \n",
    "            self.sac_buffer.append(experience)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.sac_buffer[self.ptr] = experience\n",
    "            self.ptr = (1 + self.ptr) % self.capacity\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.sac_buffer)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \n",
    "        indices = np.random.choice(len(self.sac_buffer), batch_size)\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = zip(*[self.sac_buffer[ind] for ind in indices])\n",
    "        \n",
    "        # convert to tensor\n",
    "        \n",
    "        states = torch.stack(states).to(device)\n",
    "        actions = torch.stack(actions).to(device)\n",
    "        rewards = torch.stack(rewards).to(device)\n",
    "        next_states = torch.stack(next_states).to(device)\n",
    "        dones = torch.stack(dones).to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "098f5842",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_buffer = SAC_Buffer(capacity = 500_000, batch_size = 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a91478",
   "metadata": {},
   "source": [
    "## **Rollout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "35e788ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rollout:\n",
    "    def __init__(self, world_model = world_model, actor_network = actor_network, buffer = world_buffer, sac_buffer = sac_buffer, length = 5):\n",
    "        \n",
    "        self.world = world_model\n",
    "        self.actor = actor_network\n",
    "        self.buffer = buffer\n",
    "        self.length = length\n",
    "        self.sac_buffer = sac_buffer\n",
    "        \n",
    "    def roll_out(self, batch_size):\n",
    "        \n",
    "        state, _, _, _ ,_ = self.buffer.sample(batch_size)\n",
    "        \n",
    "        #state  = torch.stack(state).to(device)\n",
    "        #reward = torch.stack(reward).to(device)\n",
    "        #next_state = torch.stack(next_state).to(device)\n",
    "        \n",
    "        for l in range(self.length):\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                action, _ = self.actor(state)\n",
    "                new_next_state, new_reward = self.world(state, action)\n",
    "                \n",
    "                new_done = torch.zeros_like(new_reward).to(device)\n",
    "                \n",
    "                if new_reward.dim() == 2:\n",
    "                    new_reward = new_reward.squeeze(-1)\n",
    "                \n",
    "                for i in range(state.shape[0]):\n",
    "                    self.sac_buffer.add(\n",
    "                        state[i].cpu().numpy(),\n",
    "                        action[i].cpu().numpy(),\n",
    "                        new_reward[i].item(),\n",
    "                        new_next_state[i].cpu().numpy(),\n",
    "                        new_done[i].item()\n",
    "                    )\n",
    "                \n",
    "                state = new_next_state.detach()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6de7392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling = Rollout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d7461f",
   "metadata": {},
   "source": [
    "# **Hyper Params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "63101835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper params\n",
    "\n",
    "gamma = 0.997\n",
    "max_iter = 10_000\n",
    "\n",
    "# Actor params\n",
    "\n",
    "actor_optimizer = optim.AdamW(actor_network.parameters(), lr = 3e-4, weight_decay = 0.001)\n",
    "actor_scheduler = optim.lr_scheduler.CosineAnnealingLR(actor_optimizer, T_max = max_iter)\n",
    "\n",
    "# Critic params\n",
    "\n",
    "critic_optimizer = optim.AdamW(critic_network.parameters(), lr = 1e-4, weight_decay = 0.001)\n",
    "critic_scheduler = optim.lr_scheduler.CosineAnnealingLR(critic_optimizer, T_max = max_iter)\n",
    "\n",
    "\n",
    "# SAC agent\n",
    "agent = SAC_Agent(actor_network, critic_network, target_critic, actor_optimizer, actor_scheduler, critic_optimizer, critic_scheduler, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedc7419",
   "metadata": {},
   "source": [
    "## **Mix Batch**\n",
    "\n",
    "This is the most important part and the ratio between real and synthetic data assummed in this implementation is 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "54708a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mix_Batch:\n",
    "    \n",
    "    def __init__(self, sac_buffer = sac_buffer, world_buffer = world_buffer, ratio = 0.25):\n",
    "        \n",
    "        self.sac_buffer = sac_buffer\n",
    "        self.world_buffer = world_buffer\n",
    "        self.ratio = ratio\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \n",
    "        real_size = int(self.ratio * batch_size)\n",
    "        actual_size = batch_size - real_size\n",
    "        \n",
    "        real_batch = self.sac_buffer.sample(real_size)\n",
    "        dyn_batch = self.world_buffer.sample(actual_size)\n",
    "        \n",
    "        r_s, r_a, r_r, r_ns, r_d = real_batch\n",
    "        d_s, d_a, d_r, d_ns, d_d = dyn_batch\n",
    "        \n",
    "        # Now Concatenate\n",
    "        \n",
    "        state = torch.cat([r_s, d_s], dim = 0).to(device)\n",
    "        action = torch.cat([r_a, d_a], dim = 0).to(device)\n",
    "        reward = torch.cat([r_r, d_r], dim = 0).to(device)\n",
    "        next_state = torch.cat([r_ns, d_ns], dim = 0).to(device)\n",
    "        done = torch.cat([r_d, d_d], dim = 0).to(device)\n",
    "        \n",
    "        return state, action, reward, next_state, done        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7bbf5685",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_batch = Mix_Batch(sac_buffer, world_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af60aa6e",
   "metadata": {},
   "source": [
    "## **Training Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b0c840b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "\n",
    "max_epochs = 300\n",
    "world_train_steps = 50\n",
    "policy_train_steps = 40\n",
    "episode_rewards = []\n",
    "ep_reward = 0\n",
    "\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551e635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(max_epochs):\n",
    "\n",
    "    #### 1. ENV INTERACTION + WORLD BUFFER FILL ###\n",
    "    \n",
    "    for _ in range(1000):\n",
    "        state, _  = env.reset()\n",
    "        action, _  = agent.select_action(state)\n",
    "        action = action.detach().cpu().numpy()[0]\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        world_buffer.add(state, action, reward, next_state, done)\n",
    "        ep_reward += reward\n",
    "\n",
    "        if done:\n",
    "            \n",
    "            episode_rewards.append(ep_reward)\n",
    "            ep_reward = 0\n",
    "\n",
    "    #### 2. WORLD MODEL UPDATE ####\n",
    "    for _ in range(world_train_steps):\n",
    "        batch = world_buffer.sample(batch_size)\n",
    "        total_wloss, state_wloss, reward_wloss = world_loss.compute_loss(*batch)\n",
    "\n",
    "    #### 3. ROLLOUT FROM WORLD MODEL ####\n",
    "    # Optional: if your rollout class returns stats\n",
    "    rolling.roll_out(batch_size)\n",
    "\n",
    "    #### 4. MIXED BATCH SAMPLING ####\n",
    "    for _ in range(policy_train_steps):\n",
    "        actor_loss, critic_loss, alpha = agent.update(mix_batch, batch_size)\n",
    "\n",
    "    #### 6. LOGGING ####\n",
    "    avg_env_reward = sum(episode_rewards) #/ len(episode_rewards)\n",
    "\n",
    "    print(f\"Epoch: {epoch}/{max_epochs}\")\n",
    "    #print(f\"  Avg Env Reward     : {avg_env_reward:.2f}\")\n",
    "    print(f\"  World Model Loss   : {total_wloss:.3f} (State: {state_wloss:.3f}, Reward: {reward_wloss:.3f})\")\n",
    "    print(f\"  SAC Losses         : Actor: {actor_loss:.3f} | Critic: {critic_loss:.3f} | Alpha: {alpha:.3f}\")\n",
    "    #print(f\"  Rollout            : {samples_added} model samples | Avg Reward: {rollout_avg_reward:.2f}\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
